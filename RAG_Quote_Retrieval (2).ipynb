{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b692211-2f41-4720-826b-4bd620c49136",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in c:\\users\\pari\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (3.6.0)\n",
      "Requirement already satisfied: sentence-transformers in c:\\users\\pari\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (4.1.0)\n",
      "Requirement already satisfied: faiss-cpu in c:\\users\\pari\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (1.11.0)\n",
      "Requirement already satisfied: transformers in c:\\users\\pari\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (4.52.3)\n",
      "Requirement already satisfied: accelerate in c:\\users\\pari\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (1.7.0)\n",
      "Requirement already satisfied: streamlit in c:\\users\\pari\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (1.44.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\pari\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from datasets) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\pari\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from datasets) (2.2.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\pari\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from datasets) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\pari\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\pari\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\pari\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\pari\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\pari\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\pari\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in c:\\users\\pari\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in c:\\users\\pari\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from datasets) (0.30.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\pari\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\pari\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\pari\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.16)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\pari\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sentence-transformers) (2.6.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\pari\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sentence-transformers) (1.6.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\pari\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sentence-transformers) (1.15.3)\n",
      "Requirement already satisfied: Pillow in c:\\users\\pari\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sentence-transformers) (11.1.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\pari\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sentence-transformers) (4.13.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\pari\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\pari\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\pari\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: psutil in c:\\users\\pari\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from accelerate) (7.0.0)\n",
      "Requirement already satisfied: altair<6,>=4.0 in c:\\users\\pari\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from streamlit) (5.5.0)\n",
      "Requirement already satisfied: blinker<2,>=1.0.0 in c:\\users\\pari\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from streamlit) (1.9.0)\n",
      "Requirement already satisfied: cachetools<6,>=4.0 in c:\\users\\pari\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from streamlit) (5.5.2)\n",
      "Requirement already satisfied: click<9,>=7.0 in c:\\users\\pari\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from streamlit) (8.1.8)\n",
      "Requirement already satisfied: protobuf<6,>=3.20 in c:\\users\\pari\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from streamlit) (5.29.4)\n",
      "Requirement already satisfied: tenacity<10,>=8.1.0 in c:\\users\\pari\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from streamlit) (9.1.2)\n",
      "Requirement already satisfied: toml<2,>=0.10.1 in c:\\users\\pari\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from streamlit) (0.10.2)\n",
      "Requirement already satisfied: watchdog<7,>=2.1.5 in c:\\users\\pari\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from streamlit) (6.0.0)\n",
      "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in c:\\users\\pari\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from streamlit) (3.1.44)\n",
      "Requirement already satisfied: pydeck<1,>=0.8.0b4 in c:\\users\\pari\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from streamlit) (0.9.1)\n",
      "Requirement already satisfied: tornado<7,>=6.0.3 in c:\\users\\pari\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from streamlit) (6.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\pari\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from altair<6,>=4.0->streamlit) (3.1.6)\n",
      "Requirement already satisfied: jsonschema>=3.0 in c:\\users\\pari\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n",
      "Requirement already satisfied: narwhals>=1.14.2 in c:\\users\\pari\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from altair<6,>=4.0->streamlit) (1.33.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\pari\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from click<9,>=7.0->streamlit) (0.4.6)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\pari\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in c:\\users\\pari\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\pari\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\pari\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\pari\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\pari\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\pari\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\pari\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\pari\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\pari\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\pari\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\pari\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\pari\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\pari\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.3.2)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\pari\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\pari\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.19.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\pari\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\pari\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\pari\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\pari\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.24.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\pari\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\pari\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\pari\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (80.7.1)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\pari\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\pari\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\pari\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.5.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\pari\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~otebook (C:\\Users\\PARI\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~otebook (C:\\Users\\PARI\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~otebook (C:\\Users\\PARI\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~otebook (C:\\Users\\PARI\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~otebook (C:\\Users\\PARI\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~otebook (C:\\Users\\PARI\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets sentence-transformers faiss-cpu transformers accelerate streamlit\n",
    "!pip install -q sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f302caf-8848-4922-b198-5740039ac0b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['quote', 'author', 'tags'],\n",
      "        num_rows: 2508\n",
      "    })\n",
      "})\n",
      "{'quote': '“Be yourself; everyone else is already taken.”', 'author': 'Oscar Wilde', 'tags': ['be-yourself', 'gilbert-perreira', 'honesty', 'inspirational', 'misattributed-oscar-wilde', 'quote-investigator']}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the Abirate/english_quotes dataset\n",
    "dataset = load_dataset(\"Abirate/english_quotes\")\n",
    "\n",
    "# Inspect dataset info and first example\n",
    "print(dataset)\n",
    "print(dataset['train'][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb9cb657-e4b2-48eb-9e20-0c79f6e5c95b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>quote</th>\n",
       "      <th>author</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>“be yourself; everyone else is already taken.”</td>\n",
       "      <td>Oscar Wilde</td>\n",
       "      <td>[be-yourself, gilbert-perreira, honesty, inspi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>“i'm selfish, impatient and a little insecure....</td>\n",
       "      <td>Marilyn Monroe</td>\n",
       "      <td>[best, life, love, mistakes, out-of-control, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>“two things are infinite: the universe and hum...</td>\n",
       "      <td>Albert Einstein</td>\n",
       "      <td>[human-nature, humor, infinity, philosophy, sc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>“so many books, so little time.”</td>\n",
       "      <td>Frank Zappa</td>\n",
       "      <td>[books, humor]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>“a room without books is like a body without a...</td>\n",
       "      <td>Marcus Tullius Cicero</td>\n",
       "      <td>[books, simile, soul]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               quote                 author  \\\n",
       "0     “be yourself; everyone else is already taken.”            Oscar Wilde   \n",
       "1  “i'm selfish, impatient and a little insecure....         Marilyn Monroe   \n",
       "2  “two things are infinite: the universe and hum...        Albert Einstein   \n",
       "3                   “so many books, so little time.”            Frank Zappa   \n",
       "4  “a room without books is like a body without a...  Marcus Tullius Cicero   \n",
       "\n",
       "                                                tags  \n",
       "0  [be-yourself, gilbert-perreira, honesty, inspi...  \n",
       "1  [best, life, love, mistakes, out-of-control, t...  \n",
       "2  [human-nature, humor, infinity, philosophy, sc...  \n",
       "3                                     [books, humor]  \n",
       "4                              [books, simile, soul]  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert to pandas dataframe for easy manipulation\n",
    "df = pd.DataFrame(dataset['train'])\n",
    "\n",
    "# Drop rows with missing 'quote' or 'author'\n",
    "df = df.dropna(subset=['quote', 'author']).reset_index(drop=True)\n",
    "\n",
    "# Lowercase quotes\n",
    "df['quote'] = df['quote'].str.lower()\n",
    "\n",
    "# Normalize tags (make lowercase, handle missing)\n",
    "df['tags'] = df['tags'].apply(lambda x: [tag.lower() for tag in x] if x else [])\n",
    "\n",
    "# Preview cleaned data\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e241bd82-0072-45df-9576-fbb1a0e01c16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example formatted text:\n",
      " “be yourself; everyone else is already taken.” - Oscar Wilde [tags: be-yourself, gilbert-perreira, honesty, inspirational, misattributed-oscar-wilde, quote-investigator]\n"
     ]
    }
   ],
   "source": [
    "# Combine quote, author, and tags into one text string per sample\n",
    "texts = [\n",
    "    f\"{row['quote']} - {row['author']} [tags: {', '.join(row['tags'])}]\"\n",
    "    for _, row in df.iterrows()\n",
    "]\n",
    "\n",
    "print(\"Example formatted text:\\n\", texts[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7fc291a3-fbc7-4949-8637-022a6c9f5457",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d858c82539d44bac8e242b32ee1ba700",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing widget examples:   0%|          | 0/1 [00:00<?, ?example/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='157' max='157' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [157/157 11:59, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, InputExample, losses\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Load base model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Prepare training data with pairs (anchor + positive = same text)\n",
    "train_examples = [InputExample(texts=[text, text]) for text in texts]\n",
    "\n",
    "train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=16)\n",
    "train_loss = losses.MultipleNegativesRankingLoss(model)\n",
    "\n",
    "# Fine-tune (1 epoch recommended for demo; increase if needed)\n",
    "model.fit(\n",
    "    train_objectives=[(train_dataloader, train_loss)],\n",
    "    epochs=1,\n",
    "    warmup_steps=100,\n",
    ")\n",
    "\n",
    "# Save model after fine-tuning\n",
    "model.save(\"fine_tuned_quote_embedding_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f1d169b-dc06-4160-a7e5-73c92f2b53fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bf250004ccb481cac44bdaf864a79bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index has 2508 vectors\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import faiss\n",
    "\n",
    "# Load fine-tuned model\n",
    "model = SentenceTransformer(\"fine_tuned_quote_embedding_model\")\n",
    "\n",
    "# Generate embeddings for all quote texts\n",
    "corpus_embeddings = model.encode(texts, convert_to_numpy=True, show_progress_bar=True)\n",
    "\n",
    "# Normalize embeddings for cosine similarity\n",
    "faiss.normalize_L2(corpus_embeddings)\n",
    "\n",
    "# Initialize FAISS index\n",
    "dimension = corpus_embeddings.shape[1]\n",
    "index = faiss.IndexFlatIP(dimension)  # Using Inner Product for cosine similarity\n",
    "index.add(corpus_embeddings)\n",
    "\n",
    "print(f\"FAISS index has {index.ntotal} vectors\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20a0b18a-de80-48ee-a9f0-062f1ecbabbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample formatted text:\n",
      " “be yourself; everyone else is already taken.” - Oscar Wilde [tags: be-yourself, gilbert-perreira, honesty, inspirational, misattributed-oscar-wilde, quote-investigator]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f7628eb7b594c63be9093a234906a7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing widget examples:   0%|          | 0/1 [00:00<?, ?example/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='157' max='157' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [157/157 15:08, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed451e3553ac42ee9ac9a57f916ceb81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated answer:\n",
      " Oscar Wilde [tags: love] “always forgive your enemies; nothing annoys them so much.”\n",
      "\n",
      "Top retrieved quotes:\n",
      "1. “some cause happiness wherever they go; others whenever they go.” - Oscar Wilde (attributed to) [tags: classic-insult] (Similarity: 0.551)\n",
      "2. “selfishness is not living as one wishes to live, it is asking others to live as one wishes to live.” - Oscar Wilde [tags: selfishness] (Similarity: 0.531)\n",
      "3. “who, being loved, is poor?” - Oscar Wilde [tags: love] (Similarity: 0.527)\n",
      "4. “always forgive your enemies; nothing annoys them so much.” - Oscar Wilde [tags: enemies, forgiveness, strategy] (Similarity: 0.503)\n",
      "5. “you will always be fond of me. i represent to you all the sins you never had the courage to commit.” - Oscar Wilde, [tags: sin] (Similarity: 0.495)\n",
      "Query: quotes about hope by Oscar Wilde\n",
      "Precision@5: 1.00\n",
      "Generated Answer: Oscar Wilde [tags: love] “always forgive your enemies; nothing annoys them so much.”\n",
      "\n",
      "Query: quotes about love and friendship\n",
      "Precision@5: 0.60\n",
      "Generated Answer: Ernest Hemingway [tags: books, friends, novelist-quotes] “friendship marks a life even more deeply than love. love risks degenerating into obsession, friendship is never anything but sharing.”\n",
      "\n",
      "Query: motivational quotes on success\n",
      "Precision@5: 0.80\n",
      "Generated Answer: motivational, positive, positive-affirmation, positive-life, positive-thinking, success\n",
      "\n",
      "Query: funny quotes by Mark Twain\n",
      "Precision@5: 1.00\n",
      "Generated Answer: [tags: humor] “never tell the truth to people who are not worthy of it.”\n",
      "\n",
      "Query: inspirational quotes on courage by women authors\n",
      "Precision@5: 0.80\n",
      "Generated Answer: courage is the most important of all the virtues because without courage, you can't practice any other virtue consistently.”\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Install required packages if not already installed\n",
    "# !pip install datasets sentence-transformers faiss-cpu transformers accelerate streamlit\n",
    "\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer, InputExample, losses\n",
    "from torch.utils.data import DataLoader\n",
    "import faiss\n",
    "import pickle\n",
    "import numpy as np\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "# ----------- 1. Load and preprocess dataset -----------\n",
    "\n",
    "dataset = load_dataset(\"Abirate/english_quotes\")\n",
    "\n",
    "df = pd.DataFrame(dataset['train'])\n",
    "\n",
    "# Drop rows with missing quote or author\n",
    "df = df.dropna(subset=['quote', 'author']).reset_index(drop=True)\n",
    "\n",
    "# Lowercase quotes and tags\n",
    "df['quote'] = df['quote'].str.lower()\n",
    "df['tags'] = df['tags'].apply(lambda x: [tag.lower() for tag in x] if x else [])\n",
    "\n",
    "# Format texts as: \"quote - author [tags: tag1, tag2]\"\n",
    "texts = [\n",
    "    f\"{row['quote']} - {row['author']} [tags: {', '.join(row['tags'])}]\"\n",
    "    for _, row in df.iterrows()\n",
    "]\n",
    "\n",
    "print(\"Sample formatted text:\\n\", texts[0])\n",
    "\n",
    "# ----------- 2. Fine-tune SentenceTransformer model -----------\n",
    "\n",
    "# Load base model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Prepare training examples (using pairs of same text for demo)\n",
    "train_examples = [InputExample(texts=[text, text]) for text in texts]\n",
    "\n",
    "train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=16)\n",
    "train_loss = losses.MultipleNegativesRankingLoss(model)\n",
    "\n",
    "# Fine-tune model (1 epoch, increase as needed)\n",
    "model.fit(\n",
    "    train_objectives=[(train_dataloader, train_loss)],\n",
    "    epochs=1,\n",
    "    warmup_steps=100,\n",
    ")\n",
    "\n",
    "# Save the fine-tuned model locally\n",
    "model.save(\"fine_tuned_quote_embedding_model\")\n",
    "\n",
    "# ----------- 3. Build FAISS index -----------\n",
    "\n",
    "# Reload fine-tuned model from local folder (optional but recommended)\n",
    "model = SentenceTransformer(\"./fine_tuned_quote_embedding_model\")\n",
    "\n",
    "# Generate embeddings for all texts\n",
    "corpus_embeddings = model.encode(texts, convert_to_numpy=True, show_progress_bar=True)\n",
    "\n",
    "# Normalize embeddings and create FAISS index\n",
    "faiss.normalize_L2(corpus_embeddings)\n",
    "dim = corpus_embeddings.shape[1]\n",
    "index = faiss.IndexFlatIP(dim)\n",
    "index.add(corpus_embeddings)\n",
    "\n",
    "# Save embeddings and texts for reuse\n",
    "with open(\"texts.pkl\", \"wb\") as f:\n",
    "    pickle.dump(texts, f)\n",
    "np.save(\"corpus_embeddings.npy\", corpus_embeddings)\n",
    "\n",
    "# ----------- 4. Load generation model and tokenizer -----------\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-small\")\n",
    "llm = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-small\")\n",
    "\n",
    "# ----------- 5. Define retrieval + generation function -----------\n",
    "\n",
    "def retrieve_and_generate(query, model, index, texts, tokenizer, llm, top_k=5):\n",
    "    query_embedding = model.encode([query], convert_to_numpy=True)\n",
    "    faiss.normalize_L2(query_embedding)\n",
    "    distances, indices = index.search(query_embedding, top_k)\n",
    "    retrieved_texts = [texts[i] for i in indices[0]]\n",
    "    context = \"\\n\".join(retrieved_texts)\n",
    "    prompt = f\"Use the following quotes to answer the query:\\n{context}\\n\\nQuery: {query}\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    outputs = llm.generate(**inputs, max_length=100)\n",
    "    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return answer, retrieved_texts, distances[0]\n",
    "\n",
    "# ----------- 6. Test retrieval and generation -----------\n",
    "\n",
    "query = \"quotes about hope by Oscar Wilde\"\n",
    "\n",
    "answer, retrieved_quotes, scores = retrieve_and_generate(\n",
    "    query, model, index, texts, tokenizer, llm\n",
    ")\n",
    "\n",
    "print(\"Generated answer:\\n\", answer)\n",
    "print(\"\\nTop retrieved quotes:\")\n",
    "for i, (quote, score) in enumerate(zip(retrieved_quotes, scores)):\n",
    "    print(f\"{i+1}. {quote} (Similarity: {score:.3f})\")\n",
    "\n",
    "# ----------- 7. Manual RAG evaluation -----------\n",
    "\n",
    "test_queries = [\n",
    "    {\"query\": \"quotes about hope by Oscar Wilde\", \"expected_author\": \"oscar wilde\"},\n",
    "    {\"query\": \"quotes about love and friendship\", \"expected_tags\": [\"love\", \"friendship\"]},\n",
    "    {\"query\": \"motivational quotes on success\", \"expected_tags\": [\"success\", \"motivation\"]},\n",
    "    {\"query\": \"funny quotes by Mark Twain\", \"expected_author\": \"mark twain\"},\n",
    "    {\"query\": \"inspirational quotes on courage by women authors\", \"expected_tags\": [\"courage\"], \"expected_author\": \"woman\"},\n",
    "]\n",
    "\n",
    "def evaluate_rag(test_queries, model, index, texts, tokenizer, llm, top_k=5):\n",
    "    results = []\n",
    "    for item in test_queries:\n",
    "        query = item[\"query\"]\n",
    "        answer, retrieved_quotes, scores = retrieve_and_generate(query, model, index, texts, tokenizer, llm, top_k=top_k)\n",
    "\n",
    "        relevant_count = 0\n",
    "        expected_author = item.get(\"expected_author\", \"\").lower()\n",
    "        expected_tags = [tag.lower() for tag in item.get(\"expected_tags\", [])]\n",
    "\n",
    "        for quote_text in retrieved_quotes:\n",
    "            quote_text_lower = quote_text.lower()\n",
    "            if expected_author and expected_author in quote_text_lower:\n",
    "                relevant_count += 1\n",
    "                continue\n",
    "            if expected_tags:\n",
    "                start = quote_text_lower.find(\"[tags:\")\n",
    "                if start != -1:\n",
    "                    tags_str = quote_text_lower[start:]\n",
    "                    for tag in expected_tags:\n",
    "                        if tag in tags_str:\n",
    "                            relevant_count += 1\n",
    "                            break\n",
    "\n",
    "        precision_at_k = relevant_count / top_k\n",
    "        results.append({\"query\": query, \"precision_at_k\": precision_at_k, \"answer\": answer})\n",
    "\n",
    "    return results\n",
    "\n",
    "# Run evaluation\n",
    "evaluation_results = evaluate_rag(test_queries, model, index, texts, tokenizer, llm, top_k=5)\n",
    "\n",
    "for res in evaluation_results:\n",
    "    print(f\"Query: {res['query']}\")\n",
    "    print(f\"Precision@5: {res['precision_at_k']:.2f}\")\n",
    "    print(f\"Generated Answer: {res['answer']}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a901773-e9b2-4996-be3a-dd68a59060ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG evaluation data exported successfully to 'rag_evaluation_export.json'\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Your list of test queries, same as used in evaluation\n",
    "test_queries = [\n",
    "    {\"query\": \"quotes about hope by Oscar Wilde\", \"expected_author\": \"oscar wilde\"},\n",
    "    {\"query\": \"quotes about love and friendship\", \"expected_tags\": [\"love\", \"friendship\"]},\n",
    "    {\"query\": \"motivational quotes on success\", \"expected_tags\": [\"success\", \"motivation\"]},\n",
    "    {\"query\": \"funny quotes by Mark Twain\", \"expected_author\": \"mark twain\"},\n",
    "    {\"query\": \"inspirational quotes on courage by women authors\", \"expected_tags\": [\"courage\"], \"expected_author\": \"woman\"},\n",
    "]\n",
    "\n",
    "results_for_export = []\n",
    "\n",
    "for item in test_queries:\n",
    "    query_text = item[\"query\"]\n",
    "    answer, retrieved_quotes, scores = retrieve_and_generate(\n",
    "        query_text, model, index, texts, tokenizer, llm, top_k=5\n",
    "    )\n",
    "\n",
    "    record = {\n",
    "        \"query\": query_text,\n",
    "        \"expected_author\": item.get(\"expected_author\", None),\n",
    "        \"expected_tags\": item.get(\"expected_tags\", []),\n",
    "        \"generated_answer\": answer,\n",
    "        \"retrieved_quotes\": retrieved_quotes,\n",
    "        \"similarity_scores\": scores.tolist() if hasattr(scores, \"tolist\") else list(scores),\n",
    "    }\n",
    "\n",
    "    results_for_export.append(record)\n",
    "\n",
    "with open(\"rag_evaluation_export.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results_for_export, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(\"RAG evaluation data exported successfully to 'rag_evaluation_export.json'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "89d80055-ba99-4b62-b3d5-deb1b46e44bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54b3df4c2af64bcba9a64908482df96e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception raised in Job[0]: KeyError(0)\n",
      "Exception raised in Job[1]: KeyError(0)\n",
      "Exception raised in Job[2]: KeyError(0)\n",
      "Exception raised in Job[3]: KeyError(0)\n",
      "Exception raised in Job[4]: KeyError(0)\n",
      "Exception raised in Job[5]: KeyError(0)\n",
      "Exception raised in Job[6]: KeyError(0)\n",
      "Exception raised in Job[7]: KeyError(0)\n",
      "Exception raised in Job[8]: KeyError(0)\n",
      "Exception raised in Job[9]: KeyError(0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context_precision: nan\n",
      "context_recall: nan\n"
     ]
    }
   ],
   "source": [
    "from ragas import SingleTurnSample, EvaluationDataset, evaluate\n",
    "from ragas.metrics import ContextPrecision, ContextRecall\n",
    "\n",
    "# Dummy LLM to avoid API calls (same as before)\n",
    "class DummyResponse:\n",
    "    def __init__(self, text):\n",
    "        self.generations = [{'text': text}]\n",
    "\n",
    "class DummyLLM:\n",
    "    async def generate(self, prompt, **kwargs):\n",
    "        return DummyResponse(\"dummy response\")\n",
    "    def set_run_config(self, run_config): pass\n",
    "    def reset(self): pass\n",
    "    def get_name(self): return \"dummy-llm\"\n",
    "\n",
    "dummy_llm = DummyLLM()\n",
    "\n",
    "# Create samples with list for retrieved_contexts (Pydantic happy)\n",
    "samples = []\n",
    "for item in test_queries:\n",
    "    query = item['query']\n",
    "    answer, retrieved_quotes, _ = retrieve_and_generate(query, model, index, texts, tokenizer, llm)\n",
    "\n",
    "    reference_text = \"\\n\".join(retrieved_quotes)\n",
    "\n",
    "    sample = SingleTurnSample(\n",
    "        user_input=query,\n",
    "        response=answer,\n",
    "        reference=reference_text,\n",
    "        retrieved_contexts=retrieved_quotes  # list here\n",
    "    )\n",
    "    samples.append(sample)\n",
    "\n",
    "# Subclass EvaluationDataset to monkey-patch retrieved_contexts to dict at runtime\n",
    "class CustomEvaluationDataset(EvaluationDataset):\n",
    "    def __getitem__(self, idx):\n",
    "        sample = super().__getitem__(idx)\n",
    "        # Convert list to dict with string keys before metrics consume it\n",
    "        if isinstance(sample.retrieved_contexts, list):\n",
    "            sample.retrieved_contexts = {str(i): v for i, v in enumerate(sample.retrieved_contexts)}\n",
    "        return sample\n",
    "\n",
    "dataset = CustomEvaluationDataset(samples)\n",
    "\n",
    "metrics = [ContextPrecision(), ContextRecall()]\n",
    "\n",
    "results = evaluate(dataset, metrics=metrics, llm=dummy_llm)\n",
    "\n",
    "for metric in metrics:\n",
    "    scores = results[metric.name]\n",
    "    avg_score = sum(scores) / len(scores) if scores else 0\n",
    "    print(f\"{metric.name}: {avg_score:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f7465d4-c297-4c70-a686-86af8af61f9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAGAS version: 0.2.15\n",
      "Available attributes in ragas module:\n",
      "['CacheInterface', 'DiskCacheBackend', 'EvaluationDataset', 'MultiTurnSample', 'RunConfig', 'SingleTurnSample', '__all__', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', '__version__', '_analytics', '_version', 'cache', 'cacher', 'callbacks', 'cost', 'dataset_schema', 'embeddings', 'evaluate', 'evaluation', 'exceptions', 'executor', 'integrations', 'llms', 'losses', 'messages', 'metrics', 'prompt', 'run_config', 'sdk', 'utils', 'validation']\n"
     ]
    }
   ],
   "source": [
    "import ragas\n",
    "print(\"RAGAS version:\", ragas.__version__)\n",
    "print(\"Available attributes in ragas module:\")\n",
    "print(dir(ragas))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5570bb-388b-49e3-8d07-1e4cb86cfffa",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
